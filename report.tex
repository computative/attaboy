\documentclass[11pt,english,a4paper]{article}
\usepackage{babel}
\input{/home/marius/Dokumenter/preamples/phys_en.pre}
\author{\normalsize Marius Jonsson (Institutt for Vanskelig Fysikk, Oscars gate 19, 0352 OSLO, Norway) \\\\
\vspace{5px}
\normalsize \texttt{http://github.com/kingoslo/attaboy}}
\title{\bf \uppercase{TiTlE}}
\date{\normalsize \today}
\addbibresource{/home/marius/Dokumenter/MyLibrary.bib}
\DeclareUnicodeCharacter{2212}{$-$}
\begin{document}
\maketitle
\begin{abstract} \normalsize This is a report submission for the fourth project of «Computational physics» at the Institute of Physics, University of Oslo, autumn 2016.
\end{abstract}
\lstset{
  xleftmargin=.2\textwidth, xrightmargin=.2\textwidth
}



\section*{\uppercase{Introduction}}
> I Motivate the reader, the first part of the introduction gives always a motivation and tries to give the overarching ideas\\
> I What I have done\\
> I The structure of the report, how it is organized etc
\section*{\uppercase{Methods}}
We used the Metropolis algorithm in conjunction with the Ising model to approximate the Boltzman distribution $P(E)$. The Metropolis algorithm works by constructing a Markov chain such that the stationary distribution is $P$ (Robert, Christian; Casella, George (2004). Monte Carlo Statistical Methods). We know that a Markov chain is defined by its transition distribution. We know that detailed balance(hva) and ergodicity(hva) are sufficient but (but not required conditions) for existance and uniqueness of stationary distribution respectively.

Let's design a markov chain such that its stationary distripution is $P$. We will call this markov chain the Metropolis algorithm. We use the detailed balance criterion using P as the asymptotic distribution. This ensures that  the resulting markov chain will have P as an asymptotic distribution.
\[
p(i,j)P(j) = p(j,i)P(i) \qquad \text{only if} \qquad \frac{P(i)}{P(j)} = \frac{p(i,j)}{p(j,i)}
\]
Now pick any conditional distribution $g:\mathbb{R}^2 \to \mathbb{R}$ and a function $A: \mathbb{R}^2 \to \mathbb{R}$ such that
\begin{equation}
p(i,j) = g(i,j)A(i,j) \qquad \text{only if} \qquad \frac{A(i,j)}{A(j,i)} = \frac{P(i)}{P(j)} \frac{g(j,i)}{g(i,j)} \label{eq:mcchain}
\end{equation}
Clearly, by fixing the transformation rules of $A$ and $g$ one uniquely defines $p$ since $p$ is the pointwise product of $A$ with $g$. Let $g$ be any function on $\mathbb{R}^2$ with non-compact support, and define $A$ by
\begin{equation}
A(i,j) = \min \left( 1, \frac{P(i)}{P(j)} \frac{g(j,i)}{g(i,j)} \right),
\end{equation}
which solves equation \eqref{eq:mcchain} (check yourself!). This is the selection rule for the Metropolis algorithm as you will find implemented in this project.\\
\\
And sinec this markov chain is ergodic (Hjorth jensen), $P$ is the only stationary distribution of of $p$ (Robert, Christian; Casella, George (2004). Monte Carlo Statistical Methods). Let us consider how this is implemented in the case of the Ising model, where $A(i,j)/A(j,i)$ is taken to be the Boltzman factor.\\
\\
In this project we assume we are working with a sequence of $L \times L$ matrices $M_n$ where the elements $m_{u,v}$ are either $1$ or $-1$. Let $J$ be some real number, we assign a quantity called the energy denoted by $E$ which is computed by
\begin{equation}
E_n = -J \sum_{(i,j) \in S_L^2} (m_{ij} m_{i(j+1 \ \mathrm{mod} \ L)} + m_{ij} m_{(i+1 \ \mathrm{mod} \ L)j}), \label{energy}
\end{equation}
letting $S_L^2 = \big\{ (u,v) \in \{1,\cdots, L \}\times\{1,\cdots, L \} \big\}$, this ensures the sum in \eqref{energy} is over all products of neighbouring elements of the matrix $M$ including periodic boundary conditions. We take the target distribution $P$ for the Metropolis algorithm to be the Boltzmann distribution. Hence $P(i) = (1/Z) e^{ -\beta E_i}$, for $\beta = (k_B T)^{  -1}$ and $k_B$ the Boltzmann constant. We let $g(i,j)$ be the unknown function induced by by the selection rule $A$ and Boltzmann factor when we flip one spin at a time. Bla bla\\
\\
The algorithm was implemented in c++ by:
\begin{lstlisting}[language=c++]
for (int k = 0; k < N; k++) { // sample iterations
    for (int i = 0; i < m; i++) { // loop matrix cols  
        for (int j = 0; j < n; j++) { // matrix rows
            int u = rand_int(gen);
            int v = rand_int(gen);
            double dE = 2*A[u][v]*(A[u][mod(v+1,n)] + 
                                       A[mod(u+1,m)][v] +
                                       A[u][mod(v-1,n)] + 
                                       A[mod(u-1,m)][v]);
            if (exp(-beta*dE) > rand_double(gen)) {
                // selection criterion                
                A[u][v] = - A[u][v];
                E += dE; 
                M += 2*A[u][v];
            }
        }
    }
    // sample if we believe we're at equilibrium
    if (k > samplepoint) {
        avg[0] += E;
        avg[1] += E*E;
        avg[2] += abs(M);
        avg[3] += M*M;
    }
}
\end{lstlisting}
We computed the exact result for $L=2$ and compared them to the numerical results. The errors were reasonable, indicating that the algorithm was implemented correctly (see table \ref{tab:errors}). The expressions for the exact results will be presented in the results.\\
\\
We would like to know when we have reached the asymptotic distribution $P$. Since we want to compute observables, this is useful because this ensures  that we know we are sampling to our expectation values from the Boltzmann distribution at any given confidence level. To test this, we will use a conventional $\chi^2$ statistical goodness-of-fit-test for the null hypothesis $H_0:$ $P(i)$ is the asymptotic distribution against $H_a:$ $P(i)$ is not the asymptotic distribution. The first markov iteration for which this is the case should be taken as the sampling point, because then we know that we are sampling from the Boltzmann distribution. We test this by at each step $n$ by assembling the asymptotic distribution by bootstrapping (Berk Devoire) and drawing samples from the empirical distribution at step $n$. We gather data by simulating $N$ times using the Metropolis algorithm. We let the algorithm continue for a long time, typically $10^6$ or $10^7$ Montecarlo cycles. We do this to ensure that the asnymptotic distribution is the Boltzman distribution. We store the measured energies at each monte carlo cycle $n$ for all the $N$ simulations. This means we have typically $10^6$ or $10^7$ candidates to the asymptotic distribution (the boltzmanm distribution). Since we did exactly $N$ simulations, we have $N$ observation from the candidates to the asymptotic distribution at each $n$. Each of these candidates are compared to asymptotic distribution. There exist statistical tests which can be used to determine when the sample is drawn from the asymptotic distribution for each $n$ at any confidence level $\alpha$. The most common is the chi-square goodness-of-fit test, which works as follows: At each $n$, divide cut the domain of the empirical distribution into $k$ intervals $I_j$ such that there are at least 5 observations in $I_j$ for all $j$. Count the number of observation in each interval, and label it by $n_j$. Then compute the probability of an observation to fall into $I_j$ from the empirical distribution. Call this probability $p_j$. It can be shown using the center limit theorem of dependent stochastic that the following observable has a known distribution:
\[
X^2 = \sum_{j=1}^k \frac{(n_j - np_j )^2}{np_j} \sim \chi^2_\nu \qquad \text{under $H_0$}.
\]
The distribution is the chi square distribution with $\nu$ degrees of freedom, where $\nu = k-1$ under the null hypothesis that the empirical distribution is equal to the empirical. In the case that the null-hypothesis is false, the value of $X^2$ will be large, and when the value of $X^2$ is larger than the $\chi^2_{\nu, 1-\alpha}$-quantile in the $\chi^2$-distribution, we know that the distributions are difference at confidence level $\alpha$. We chose a confidence level of $0.5$, which implies we will only accept distributions which are so similar to the asymptotic distribution, that the probability of evaluating a distribution violating the null more hypothesis more strongly is exactly 50\%.
\section*{\uppercase{Results and discussion}}
Resultater
We start by presenting our main result. By making simulating for a long time on the available cluster, we were able to simulate the same markov chain exactly 248 times for each temperature $T = 2,2.1,\cdots,2.4$ and for dimensions $L=20,60,100,140$ for exactly $10^6$ each run. With 248 samples from the distribution of $E$ for each iteration, we were able to construct the empirical distribution of $E$ at each iteration, giving us in total $1.8\cdot 10^7$ probability distributions of $E$ which could be candidates to the Boltzmann distribution. We automated the Goodness of fit test described in the methods, to discriminate which of these distributions were equal to the Boltzmann distribution at confidence level 95\%. Using this information, we found the optimal sampling point given starting conditions (see figure \ref{fig:sampl}). Table \ref{tab:sprand} gives the ideal starting points for these conditions. This could be further generalized using the method of maximum likelihood. \\
\\
We found that for starting matrix of uniformly distributed spins, the temperature was unimportant for the ideal sampling point for $L=20$, but was increasing for each temperature. An increase in temperature of 1 would reduce the sampling point by approximately one fourth. However, sampling point was strongly positively correlated with temperature for larger values of $L$. For example, for $L=140$ the ideal sampling point was shifted by exactly four order of magnitudes by a shift in temperature of 0.5 (see table \ref{tab:sprand}). This is reflected in the mean ideal sampling point given by $S = 1.3\cdot 10^4$ with standard error of $3.1\cdot 10^4$. Si noe om random eller equal er bedre, eller ingen rolle.\\
\\
\begin{figure}[!h]
\center
\input{sampling.tex}
\caption{Subfigure (a) displays variance of some empirical distribution as a function of iteration number $n$ for some temperature and starting matrix. We found that at a confidence level of 95\%, the Markov chain arrived at the steady state distribution at the point marked by the dashed line. This is at $n = S$. (b) The $p$-values of the $\chi^2$-test notices that steady state has been reached and shifts from low to high at confidence level at the suggested sampling point $S$. (c) The steady asymptotic distribution at $n = 10^7$. This is the Boltzmann distribution. (d) The empirical distribution at confidence level $95\%$ marked by the dashed line sampled at the point $n=S$. Notice that the empirical distribution is similar to the asymptotic distribution at this point since we are $95\%$ certain that they are equal.}\label{fig:sampl}
\end{figure}
\begin{table}[!h]
\center
\begin{tabular}{l c c c c c c}
$L$ & \multicolumn{5}{c}{$k_BT$} \\
		&2					&2.1			&2.2			&2.3			&2.4\\
\hline
20  	&$2.7\cdot 10^2$	&$2.4\cdot 10^2$&$1.9\cdot 10^2$&$1.1\cdot 10^2$&$1.3\cdot 10^1$\\
60		&$5.3\cdot 10^3$	&$3.6\cdot 10^3$&$2.5\cdot 10^3$&$7.6\cdot 10^2$&$9.0\cdot 10^1$\\
100		&$1.7\cdot 10^4$	&$1.7\cdot 10^4$&$6.5\cdot 10^3$&$9.2\cdot 10^2$&$8.3\cdot 10^1$\\
140		&$1.3\cdot 10^5$	&$5.8\cdot 10^4$&$1.5\cdot 10^4$&$1.3\cdot 10^3$&$1.2\cdot 10^2$
\end{tabular}
\caption{Points where $p > \alpha$, choses sampling points. Random starting matrix} \label{tab:sprand}
\end{table}%
Despite our first findings, it was mandatory to simulate for $L=20$ and two temperatures $T=1, 2.4$. We plotted the means of energy $\E{E}$ and magnetization $\E{M}$ and number of accepts per monte carlo cycles. We found that in the case of $T=1$, the ordered matrix started its markov chain near the asymptotic distribution. The means $\E{E}$ and $\E{M}_n$ was nearly constant. This was also the case for the acceptance numbers. In the case of a random starting point it appeared that approximately $10^3$ monte carlo cycles was necessary to achieve constant acceptance rate. The estimators $\E{E}$ and $\E{M}$ had an error smaller than $1\%$ after roughly $10^4$ iterations. In either case, the asymptotic distribution $P(E)$ appear Boltzmann with parameters $(\beta,n) = (1,400)$. In this case, the variance of energy was $9.48$ yielding a heat capacity per spin of 0.024 (in units of $k_B$).
In the case of $T = 2.4$, the acceptance rates of both an ordered and random starting point were approximately constant with standard deviation roughly 60. The distribution appeared roughly Boltzmann with parameters approximately $(\beta,n) \approx (100,400)$. This yielded a variance of $3226.65$ and consequently a heat capacity per spin of 1.40. The errors on the values of $\E{E}$ and $\E{M}$ was smaller than $1 \%$ at approximately $n = 10^5$. See figure \ref{fig:signlerun} for the plots.\\
\\
We found exact expressions for the partition function and Boltzmann distribution for our model in the case $L=2$. Using the definition, we obtained the partition function:
\[
Z(\beta) = 12 + 2\left( e^{ -8 J\beta} + e^{ 8 J\beta} \right)
\]
and thus the $n$th momentums of energy $E$ and magnetization $M$ are given by
\[
\E{E^n}(\beta) = \frac{2}{Z}\left( 8^n e^{ -8 J\beta} + (-8)^n e^{ 8 J\beta}  \right), \quad \E{M^n}(\beta) = \frac{1}{Z}\left( 4^n e^{ 8 J\beta} + 4(2)^n + 4(-2)^n + (-4)^n e^{ 8 J\beta}  \right)
\]
respectively. From these we derived the expressions for expected energy and expected magnitude of magnetization 
\[
\E{E}(\beta) = \frac{2^4}{Z}\left( e^{ -8 J\beta} - e^{ 8 J\beta}  \right) \qquad \text{and} \qquad \E{|M|}(J\beta) = \frac{2^3}{Z}\left(  2 + e^{ 8 J\beta} \right).
\]
Then again, use these, it was straight forward to compute the heat capacity at constant volume and magnetic susceptibility since they were
\begin{align*}
C_V &= \frac{1}{kT^2}\sigma_E^2 = \frac{2^7}{kT^2Z}\left[  \left( e^{ -8J\beta} + e^{ 8J\beta} \right) - \frac{2}{Z}\left( e^{ -8J\beta} - e^{ 8J\beta} \right)^2  \right] \\
\chi &= \frac{1}{kT} \left(\E{M^2} -\E{|M|}^2 \right) = \frac{2^5}{kTZ} \left[ \left( 1 + e^{ 8J \beta} \right) - \frac{2}{Z}\left( 2 + e^{ 8J \beta} \right)^2 \right]
\end{align*}
In the case $T=1$, we found that these estimate resonated with the numerical results, given in table \ref{tab:errors}.
\begin{table}[!h]
\center
\begin{tabular}{c l l l}
Observable&Exact:					& Numerical, $n = 10^8$& Error\\
\hline
$\E{E}$ & -7.9839283437467596		& -7.98379632& $-1.3\cdot 10^{-4}$\\
$\E{|M|}$ & 3.9946429309943987		& 3.9945928& $5.0\cdot 10^{-5}$\\
$C_V$ & 0.12832932745714487	& 0.129378400754& $-1.0\cdot 10^{-3}$\\
$\chi$ & 0.01604295806490974		& 0.0162110021882& $-1.7\cdot 10^{-4}$
\end{tabular}
\caption{BLa bla} \label{tab:errors}
\end{table}%

\begin{figure}
\center
\input{2020.pgf}
\caption{BLa bla Observables plotted against thermal energy for exactly three values of $L$. The solid line represent observables at $L=140$, whilst the dotted, dashed and double-dot-dashed lines represent $L = 100,60.20$ respectively.The solid line represent observables at $L=140$, whilst the dotted, dashed and double-dot-dashed lines represent $L = 100,60.20$ respectively. Bla bla} \label{fig:signlerun}
\end{figure}%
By simulating for 15 hours on 248 cores we found estimates of $\E{E}$,$\E{|M|}$,$C_V$ and $\chi$ for $L=20,60,100,140$ for $201$ temperatures in the interval $[2,2.4]$. We found that for $L=20$, the expected energy as a function of thermal energy was approximately linear (see figure \ref{fig:observables}). However, as the periodicity increased to say $L=140$, the thermal energy increased near the temperature $T_0=2.25$. We discovered a magnetic transient at the same temperature to lower levels of magnetization. By considering a periodicity of $L=140$, the difference in magnetization was nearly an order of magnitude relative to $L=20$ at $T_0$. The magnetic susceptibility and heat capacity had global maxima near $T_0$. It appeared $C_V$ may have a pole near $T_0$ in the limit $L \to \infty$ (see figure \ref{fig:observables}).\\
\\


> samplingspunktanalyse
> regresjonsformel (bruk random som faktor) vent til resultat kommer
> L = 20, T=1, 2.4.Plot mean E mean M ,accepts both random and ordered.
> Analytical expr 2x2. Periodic boundry
> Write code to compute numerical. T = 1 give table


fys
> Plott E, M X Cv for L=20,60,100,140, t in 2 til 2.4
> Beregn kritisk temperatur
> Beregn P(E) for T = 1, 2.4 og L=20. Finn variansen fra fordelingene. 

Diskusjon
> samplingspunktanalyse
> regresjon
> L = 20, T=1, 2.4.Accepts both random and ordered: How do accepts behave as function of T? (INTERESSANT!) Study how long before equilibrium. Can we use this to find equilibrium time?  How many cycles before equilibrium?
> Kan vi se tegn til faseovergang?
> Diskuter oppførselen til P(E) som funksjon av temp. for T = 1, 2.4, L =20
> Numerics: 2x2 How many iterations n is necessary for good fit? 




\begin{figure}
\center
\input{obs1.pgf}\\
\input{obs2.pgf}
\caption{Observables plotted against thermal energy for exactly three values of $L$. The solid line represent observables at $L=140$, whilst the dotted, dashed and double-dot-dashed lines represent $L = 100,60.20$ respectively.} \label{fig:observables}
\end{figure}





> I Present your results\\
> I Give a critical discussion of your work and place it in the correct context.\\
> I Relate your work to other calculations/studies\\
> I An eventual reader should be able to reproduce your calculations if she/he wants to do so. All input variables should be properly explained.\\
> I Make sure that figures and tables should contain enough information in their captions, axis labels etc so that an eventual reader can gain a first impression of your work by studying figures and tables only.
\section*{\uppercase{Conclusion and perspectives}}
> I State your main findings and interpretations\\
> I Try as far as possible to present perspectives for future work\\
> I Try to discuss the pros and cons of the methods and possible improvements

\printbibliography
\end{document}